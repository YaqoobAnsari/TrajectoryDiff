{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadioMapSeer Dataset Exploration\n",
    "\n",
    "This notebook explores the RadioMapSeer dataset to understand:\n",
    "1. Data format and structure\n",
    "2. Value distributions (pathloss ranges)\n",
    "3. City map characteristics\n",
    "4. Transmitter locations\n",
    "5. Preparation for trajectory sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Dataset path\n",
    "DATA_DIR = Path.cwd().parent / 'data' / 'raw' / 'RadioMapSeer'\n",
    "print(f\"Looking for data in: {DATA_DIR}\")\n",
    "print(f\"Data directory exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Structure Discovery\n",
    "\n",
    "First, let's explore what files and folders exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_directory(path: Path, max_depth: int = 3, current_depth: int = 0) -> dict:\n",
    "    \"\"\"Recursively explore directory structure.\"\"\"\n",
    "    if not path.exists() or current_depth >= max_depth:\n",
    "        return {}\n",
    "    \n",
    "    result = {\n",
    "        'files': [],\n",
    "        'dirs': {}\n",
    "    }\n",
    "    \n",
    "    for item in sorted(path.iterdir())[:20]:  # Limit to first 20 items\n",
    "        if item.is_file():\n",
    "            result['files'].append(item.name)\n",
    "        elif item.is_dir():\n",
    "            result['dirs'][item.name] = explore_directory(item, max_depth, current_depth + 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "if DATA_DIR.exists():\n",
    "    structure = explore_directory(DATA_DIR)\n",
    "    print(\"Dataset structure (first 20 items per level):\")\n",
    "    print(json.dumps(structure, indent=2))\n",
    "else:\n",
    "    print(\"⚠️  Dataset not found! Please download first:\")\n",
    "    print(\"   python scripts/download_data.py --method manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all files by type\n",
    "def count_files(path: Path) -> dict:\n",
    "    \"\"\"Count files by extension.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    total_size = 0\n",
    "    \n",
    "    if not path.exists():\n",
    "        return counts, 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            ext = Path(f).suffix.lower()\n",
    "            counts[ext] += 1\n",
    "            total_size += (Path(root) / f).stat().st_size\n",
    "    \n",
    "    return dict(counts), total_size / (1024**3)  # Size in GB\n",
    "\n",
    "if DATA_DIR.exists():\n",
    "    file_counts, total_gb = count_files(DATA_DIR)\n",
    "    print(\"File counts by extension:\")\n",
    "    for ext, count in sorted(file_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {ext or '(no ext)'}: {count:,}\")\n",
    "    print(f\"\\nTotal size: {total_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Visualize Sample Data\n",
    "\n",
    "Let's load a few samples to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sample_files(data_dir: Path, pattern: str = \"*.png\", n: int = 5) -> list:\n",
    "    \"\"\"Find sample files matching pattern.\"\"\"\n",
    "    files = list(data_dir.rglob(pattern))[:n]\n",
    "    return files\n",
    "\n",
    "# Find PNG files\n",
    "if DATA_DIR.exists():\n",
    "    png_files = find_sample_files(DATA_DIR, \"*.png\", 10)\n",
    "    print(f\"Found {len(png_files)} sample PNG files:\")\n",
    "    for f in png_files:\n",
    "        print(f\"  {f.relative_to(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_png(filepath: Path) -> dict:\n",
    "    \"\"\"Load a PNG and analyze its properties.\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    return {\n",
    "        'path': filepath.name,\n",
    "        'shape': arr.shape,\n",
    "        'dtype': str(arr.dtype),\n",
    "        'mode': img.mode,\n",
    "        'min': arr.min(),\n",
    "        'max': arr.max(),\n",
    "        'mean': arr.mean(),\n",
    "        'std': arr.std(),\n",
    "        'unique_values': len(np.unique(arr)),\n",
    "        'array': arr\n",
    "    }\n",
    "\n",
    "# Analyze sample files\n",
    "if DATA_DIR.exists() and png_files:\n",
    "    print(\"Analyzing sample PNG files:\\n\")\n",
    "    for f in png_files[:5]:\n",
    "        info = load_and_analyze_png(f)\n",
    "        print(f\"File: {info['path']}\")\n",
    "        print(f\"  Shape: {info['shape']}, Mode: {info['mode']}, Dtype: {info['dtype']}\")\n",
    "        print(f\"  Values: min={info['min']}, max={info['max']}, mean={info['mean']:.2f}\")\n",
    "        print(f\"  Unique values: {info['unique_values']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "if DATA_DIR.exists() and png_files:\n",
    "    fig, axes = plt.subplots(2, min(3, len(png_files)), figsize=(15, 10))\n",
    "    axes = axes.flatten() if len(png_files) > 1 else [axes]\n",
    "    \n",
    "    for ax, fpath in zip(axes, png_files[:6]):\n",
    "        img = np.array(Image.open(fpath))\n",
    "        if len(img.shape) == 2:  # Grayscale\n",
    "            im = ax.imshow(img, cmap='viridis')\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "        else:  # RGB\n",
    "            ax.imshow(img)\n",
    "        ax.set_title(fpath.name[:30], fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Dataset', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand City Map / Floor Plan Encoding\n",
    "\n",
    "The city maps should show buildings vs streets. Let's understand the color encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_city_map_colors(img: np.ndarray) -> dict:\n",
    "    \"\"\"Analyze color distribution in city map.\"\"\"\n",
    "    if len(img.shape) == 2:\n",
    "        # Grayscale\n",
    "        unique, counts = np.unique(img, return_counts=True)\n",
    "        return {'type': 'grayscale', 'unique_values': len(unique), 'top_values': list(zip(unique[:10], counts[:10]))}\n",
    "    \n",
    "    # RGB\n",
    "    pixels = img.reshape(-1, img.shape[-1])\n",
    "    unique, counts = np.unique(pixels, axis=0, return_counts=True)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_idx = np.argsort(-counts)\n",
    "    top_colors = [(tuple(unique[i]), counts[i]) for i in sorted_idx[:10]]\n",
    "    \n",
    "    return {\n",
    "        'type': 'rgb',\n",
    "        'unique_colors': len(unique),\n",
    "        'top_colors': top_colors\n",
    "    }\n",
    "\n",
    "# Look for city map files\n",
    "if DATA_DIR.exists():\n",
    "    # Try to find city/building map files\n",
    "    city_patterns = ['*city*.png', '*map*.png', '*building*.png', '*floor*.png']\n",
    "    city_files = []\n",
    "    for pattern in city_patterns:\n",
    "        city_files.extend(list(DATA_DIR.rglob(pattern))[:5])\n",
    "    \n",
    "    print(f\"Found {len(city_files)} potential city map files\")\n",
    "    for f in city_files[:3]:\n",
    "        print(f\"  {f.relative_to(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pathloss Value Distribution\n",
    "\n",
    "According to documentation:\n",
    "- Min pathloss: -186 dB\n",
    "- Max pathloss: -47 dB\n",
    "- PNG encoding: Linear mapping to 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversion functions based on documentation\n",
    "PL_MIN = -186  # dB\n",
    "PL_MAX = -47   # dB\n",
    "PL_RANGE = PL_MAX - PL_MIN  # 139 dB\n",
    "\n",
    "def png_to_db(png_value: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert PNG grayscale (0-255) to pathloss in dB.\"\"\"\n",
    "    return (png_value / 255.0) * PL_RANGE + PL_MIN\n",
    "\n",
    "def db_to_png(db_value: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert pathloss in dB to PNG grayscale (0-255).\"\"\"\n",
    "    return np.clip((db_value - PL_MIN) / PL_RANGE * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Test conversion\n",
    "test_png = np.array([0, 127, 255])\n",
    "test_db = png_to_db(test_png)\n",
    "print(\"PNG to dB conversion test:\")\n",
    "print(f\"  PNG values: {test_png}\")\n",
    "print(f\"  dB values:  {test_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pathloss distribution across multiple maps\n",
    "if DATA_DIR.exists() and png_files:\n",
    "    # Find files that look like radio/pathloss maps (grayscale)\n",
    "    radio_maps = []\n",
    "    for f in find_sample_files(DATA_DIR, \"*.png\", 50):\n",
    "        img = np.array(Image.open(f))\n",
    "        if len(img.shape) == 2 and img.shape == (256, 256):  # Grayscale, correct size\n",
    "            radio_maps.append(img)\n",
    "            if len(radio_maps) >= 20:\n",
    "                break\n",
    "    \n",
    "    if radio_maps:\n",
    "        all_values = np.concatenate([m.flatten() for m in radio_maps])\n",
    "        all_db = png_to_db(all_values)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # PNG value histogram\n",
    "        axes[0].hist(all_values, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_xlabel('PNG Value (0-255)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Distribution of PNG Values')\n",
    "        \n",
    "        # dB value histogram\n",
    "        axes[1].hist(all_db, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "        axes[1].set_xlabel('Pathloss (dB)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Distribution of Pathloss Values')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Analyzed {len(radio_maps)} radio maps\")\n",
    "        print(f\"PNG range: [{all_values.min()}, {all_values.max()}]\")\n",
    "        print(f\"dB range: [{all_db.min():.1f}, {all_db.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walkable Area Extraction (For Trajectory Sampling)\n",
    "\n",
    "To generate trajectories, we need to identify walkable areas (streets) vs obstacles (buildings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_walkable_mask(city_map: np.ndarray, method: str = 'threshold') -> np.ndarray:\n",
    "    \"\"\"Extract walkable areas from city map.\n",
    "    \n",
    "    Args:\n",
    "        city_map: RGB or grayscale city map\n",
    "        method: Extraction method ('threshold', 'color')\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask where 1 = walkable, 0 = obstacle\n",
    "    \"\"\"\n",
    "    if len(city_map.shape) == 3:\n",
    "        # Convert to grayscale\n",
    "        gray = np.mean(city_map, axis=-1)\n",
    "    else:\n",
    "        gray = city_map\n",
    "    \n",
    "    if method == 'threshold':\n",
    "        # Simple threshold - streets are usually lighter\n",
    "        # This is a placeholder - actual threshold depends on dataset encoding\n",
    "        threshold = np.percentile(gray, 50)\n",
    "        mask = (gray > threshold).astype(np.uint8)\n",
    "    else:\n",
    "        # TODO: Color-based extraction for RGB maps\n",
    "        mask = np.ones_like(gray, dtype=np.uint8)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Test on a sample\n",
    "if DATA_DIR.exists() and png_files:\n",
    "    # Try with first RGB image or city map\n",
    "    for f in png_files:\n",
    "        img = np.array(Image.open(f))\n",
    "        if len(img.shape) == 3:  # RGB\n",
    "            mask = extract_walkable_mask(img)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title('Original City Map')\n",
    "            axes[1].imshow(mask, cmap='gray')\n",
    "            axes[1].set_title('Walkable Mask (preliminary)')\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics\n",
    "\n",
    "Compile key statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_summary(data_dir: Path) -> dict:\n",
    "    \"\"\"Compute comprehensive dataset summary.\"\"\"\n",
    "    summary = {\n",
    "        'exists': data_dir.exists(),\n",
    "        'total_files': 0,\n",
    "        'png_files': 0,\n",
    "        'json_files': 0,\n",
    "        'total_size_gb': 0,\n",
    "        'image_shapes': set(),\n",
    "        'cities': set(),\n",
    "    }\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        return summary\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        # Try to identify cities from directory names\n",
    "        for d in dirs:\n",
    "            if d.lower() in ['ankara', 'berlin', 'glasgow', 'ljubljana', 'london', 'telaviv', 'tel_aviv']:\n",
    "                summary['cities'].add(d)\n",
    "        \n",
    "        for f in files:\n",
    "            fpath = Path(root) / f\n",
    "            summary['total_files'] += 1\n",
    "            summary['total_size_gb'] += fpath.stat().st_size / (1024**3)\n",
    "            \n",
    "            if f.endswith('.png'):\n",
    "                summary['png_files'] += 1\n",
    "                # Sample some to get shapes\n",
    "                if summary['png_files'] <= 10:\n",
    "                    img = Image.open(fpath)\n",
    "                    summary['image_shapes'].add(img.size)\n",
    "            elif f.endswith('.json'):\n",
    "                summary['json_files'] += 1\n",
    "    \n",
    "    summary['cities'] = list(summary['cities'])\n",
    "    summary['image_shapes'] = list(summary['image_shapes'])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "if DATA_DIR.exists():\n",
    "    summary = compute_dataset_summary(DATA_DIR)\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files: {summary['total_files']:,}\")\n",
    "    print(f\"PNG files: {summary['png_files']:,}\")\n",
    "    print(f\"JSON files: {summary['json_files']:,}\")\n",
    "    print(f\"Total size: {summary['total_size_gb']:.2f} GB\")\n",
    "    print(f\"Image shapes: {summary['image_shapes']}\")\n",
    "    print(f\"Cities found: {summary['cities']}\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please download first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Based on the exploration above:\n",
    "\n",
    "1. **Floor Plan Processing**: Implement proper walkable area extraction based on observed color encoding\n",
    "2. **Trajectory Generation**: Generate realistic pedestrian trajectories on streets\n",
    "3. **Data Pipeline**: Create PyTorch Dataset that loads maps and generates trajectories\n",
    "4. **Visualization**: Create overlay visualizations of trajectories on city maps\n",
    "\n",
    "See notebook `02_trajectory_visualization.ipynb` for trajectory examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")\n",
    "print(\"\\nKey findings to document:\")\n",
    "print(\"- [ ] Actual file structure and naming convention\")\n",
    "print(\"- [ ] Color encoding for city maps (buildings vs streets)\")\n",
    "print(\"- [ ] Pathloss value range in actual data\")\n",
    "print(\"- [ ] Transmitter location format\")\n",
    "print(\"- [ ] Any preprocessing needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
